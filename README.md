# ğŸ“Š Data Pipeline Streaming

[![Python](https://img.shields.io/badge/Python-3.11-blue.svg)](https://www.python.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-Latest-red.svg)](https://streamlit.io/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![GitHub](https://img.shields.io/badge/GitHub-Nedim7050-black.svg)](https://github.com/Nedim7050)

> **Pipeline de donnÃ©es de bout-en-bout** pour la gÃ©nÃ©ration, l'ingestion, la transformation et la visualisation de transactions financiÃ¨res synthÃ©tiques en temps rÃ©el.

## ğŸ‘¤ Auteur

**Nedim Mejri**  
ğŸ“§ [GitHub Profile](https://github.com/Nedim7050) | ğŸš€ [Repository](https://github.com/Nedim7050/data-pipeline-streaming)

---

## ğŸ¯ Vue d'ensemble

Ce projet implÃ©mente un **pipeline de donnÃ©es complet** (end-to-end) qui simule le traitement de transactions financiÃ¨res en temps rÃ©el. Il dÃ©montre les concepts modernes de traitement de donnÃ©es avec :

- ğŸ”„ **GÃ©nÃ©ration de donnÃ©es** : Transactions financiÃ¨res synthÃ©tiques rÃ©alistes
- ğŸ“¥ **Ingestion** : Kafka ou fichiers JSONL
- ğŸ”§ **Transformation ETL** : Apache Airflow ou scripts Python
- ğŸ’¾ **Stockage** : PostgreSQL ou SQLite
- ğŸ“Š **Visualisation** : Dashboard Streamlit interactif

### âœ¨ CaractÃ©ristiques principales

- âœ… Architecture modulaire et extensible
- âœ… Support Docker pour dÃ©ploiement facile
- âœ… Version simplifiÃ©e sans Docker (SQLite)
- âœ… Dashboard interactif avec filtres avancÃ©s
- âœ… Export des donnÃ©es (CSV)
- âœ… Documentation complÃ¨te et guides dÃ©taillÃ©s

---

## ğŸš€ DÃ©marrage Rapide

### Option 1 : Version SimplifiÃ©e (Sans Docker) â­ **RECOMMANDÃ‰**

Parfait pour dÃ©marrer rapidement sans configuration complexe.

```powershell
# 1. Installer les dÃ©pendances
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -r requirements-simple.txt

# 2. CrÃ©er la base de donnÃ©es et gÃ©nÃ©rer des donnÃ©es
python create_database.py

# 3. Lancer le dashboard
streamlit run dashboard_working.py
```

Le dashboard sera accessible sur `http://localhost:8501`

### Option 2 : Version ComplÃ¨te (Avec Docker)

Pour une architecture complÃ¨te avec Kafka, Airflow et PostgreSQL.

```bash
# 1. DÃ©marrer l'Ã©cosystÃ¨me (Kafka, Zookeeper, Airflow, Postgres)
docker compose up -d

# 2. AccÃ©der Ã  l'interface Airflow (http://localhost:8080)
# Activer le DAG 'transactions_etl'

# 3. Produire des Ã©vÃ©nements
python producer/producer.py --rows 10000 --rate 100

# 4. Visualiser dans le dashboard
streamlit run analytics/streamlit_dashboard.py
```

---

## ğŸ“Š Architecture

Le pipeline suit une architecture modulaire en plusieurs Ã©tapes :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Producer   â”‚â”€â”€â”€â”€â–¶â”‚  Kafka/JSONL â”‚â”€â”€â”€â”€â–¶â”‚    ETL      â”‚â”€â”€â”€â”€â–¶â”‚  Database   â”‚
â”‚  (Python)   â”‚     â”‚  (Queue)     â”‚     â”‚ (Airflow)   â”‚     â”‚ (Postgres/  â”‚
â”‚             â”‚     â”‚              â”‚     â”‚             â”‚     â”‚  SQLite)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                      â”‚
                                                                      â–¼
                                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                              â”‚  Streamlit  â”‚
                                                              â”‚  Dashboard  â”‚
                                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Composants

| Composant | Description | Technologie |
|-----------|-------------|-------------|
| **Producteur** | GÃ©nÃ¨re des transactions synthÃ©tiques | Python |
| **Queue** | SystÃ¨me de messagerie distribuÃ© | Kafka / JSONL |
| **ETL** | Transformation et chargement des donnÃ©es | Airflow / Python |
| **Base de donnÃ©es** | Stockage structurÃ© | PostgreSQL / SQLite |
| **Dashboard** | Visualisation interactive | Streamlit |

---

## ğŸ“ Structure du Projet

```
data-pipeline-streaming/
â”œâ”€â”€ producer/              # GÃ©nÃ©rateur de transactions synthÃ©tiques
â”‚   â”œâ”€â”€ producer.py        # Producteur Kafka
â”‚   â””â”€â”€ producer_to_file.py # Producteur fichier JSONL
â”œâ”€â”€ consumers/             # Consumers Kafka â†’ Database
â”‚   â”œâ”€â”€ kafka_to_postgres.py
â”‚   â””â”€â”€ file_queue_to_sqlite.py
â”œâ”€â”€ airflow_dags/          # DAGs Apache Airflow
â”‚   â””â”€â”€ etl_dag.py
â”œâ”€â”€ analytics/             # Dashboards Streamlit
â”‚   â”œâ”€â”€ streamlit_dashboard.py
â”‚   â””â”€â”€ streamlit_dashboard_sqlite.py
â”œâ”€â”€ scripts/               # Scripts utilitaires
â”œâ”€â”€ sql/                   # SchÃ©mas de base de donnÃ©es
â”œâ”€â”€ docker/                # Configuration Docker
â”œâ”€â”€ notebooks/             # Notebooks d'exploration
â”œâ”€â”€ create_database.py     # Script de crÃ©ation de base de donnÃ©es
â”œâ”€â”€ dashboard_working.py   # Dashboard Streamlit principal
â”œâ”€â”€ streamlit_app.py       # App Streamlit pour Cloud
â”œâ”€â”€ requirements.txt       # DÃ©pendances (version complÃ¨te)
â””â”€â”€ requirements-simple.txt # DÃ©pendances (version simplifiÃ©e)
```

---

## ğŸ”§ Configuration

### Variables d'environnement

```bash
# Kafka
KAFKA_BOOTSTRAP_SERVER=localhost:29092
KAFKA_TOPIC=transactions

# PostgreSQL (version complÃ¨te)
POSTGRES_CONN_URI=postgresql+psycopg2://airflow:airflow@localhost:5432/transactions

# SQLite (version simplifiÃ©e)
SQLITE_DB_PATH=data/transactions.db
```

---

## ğŸ“Š FonctionnalitÃ©s du Dashboard

Le dashboard Streamlit offre :

- ğŸ“ˆ **MÃ©triques en temps rÃ©el** : Montants, transactions, statistiques
- ğŸ“Š **Graphiques interactifs** : Visualisations des donnÃ©es
- ğŸ” **Filtres avancÃ©s** : CatÃ©gorie, ville, statut, montant, date
- ğŸ“¥ **Export CSV** : Export des donnÃ©es pour analyse externe
- ğŸ¨ **Interface intuitive** : Design moderne et responsive

---

## ğŸŒ DÃ©ploiement

### DÃ©ploiement sur Streamlit Cloud

âœ… **Le projet est dÃ©jÃ  disponible sur GitHub** : [Nedim7050/data-pipeline-streaming](https://github.com/Nedim7050/data-pipeline-streaming)

1. **Aller sur [Streamlit Cloud](https://streamlit.io/cloud)**
2. **Se connecter avec GitHub**
3. **Cliquer sur "New app"**
4. **Configurer le dÃ©ploiement :**
   - **Repository :** `Nedim7050/data-pipeline-streaming`
   - **Branch :** `main`
   - **Main file :** `streamlit_app.py`
   - **Python version :** `3.11`
5. **Cliquer sur "Deploy!"**

ğŸ“– Voir [DEPLOY.md](DEPLOY.md) ou [GITHUB-DEPLOY.md](GITHUB-DEPLOY.md) pour plus de dÃ©tails.

---

## ğŸ“š Documentation

- ğŸ“– [PRESENTATION.md](PRESENTATION.md) - PrÃ©sentation complÃ¨te du projet, objectifs et cas d'usage
- âš¡ [QUICKSTART.md](QUICKSTART.md) - Guide de dÃ©marrage rapide
- ğŸ“Š [GUIDE-ANALYSE-NOUVELLE-BD.md](GUIDE-ANALYSE-NOUVELLE-BD.md) - Guide pour analyser une nouvelle base de donnÃ©es
- ğŸš€ [DEPLOY.md](DEPLOY.md) - Guide de dÃ©ploiement dÃ©taillÃ©
- ğŸŒ [GITHUB-DEPLOY.md](GITHUB-DEPLOY.md) - Guide complet GitHub et Streamlit Cloud
- ğŸ¯ [START-HERE.md](START-HERE.md) - Guide de dÃ©marrage pour dÃ©butants
- âœ¨ [AMELIORATIONS-PROJET.md](AMELIORATIONS-PROJET.md) - AmÃ©liorations et fonctionnalitÃ©s futures

---

## ğŸ› ï¸ Technologies UtilisÃ©es

- **Python 3.11** - Langage principal
- **Streamlit** - Dashboard interactif
- **Apache Kafka** - SystÃ¨me de messagerie distribuÃ©
- **Apache Airflow** - Orchestration de workflows
- **PostgreSQL** - Base de donnÃ©es relationnelle
- **SQLite** - Base de donnÃ©es lÃ©gÃ¨re
- **Docker** - Containerisation
- **Pandas** - Manipulation de donnÃ©es
- **Plotly** - Visualisations interactives

---

## ğŸ“Š Cas d'Usage

- ğŸ“ **Ã‰tudiants** : Apprendre les concepts de pipelines de donnÃ©es
- ğŸ’¼ **DÃ©veloppeurs** : Prototyper des systÃ¨mes de traitement de donnÃ©es
- ğŸ¢ **Entreprises** : DÃ©monstration de concepts de data engineering
- ğŸ“š **Formateurs** : MatÃ©riel pÃ©dagogique pour cours de donnÃ©es

---

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  :

- ğŸ› Signaler des bugs
- ğŸ’¡ Proposer de nouvelles fonctionnalitÃ©s
- ğŸ“ AmÃ©liorer la documentation
- ğŸ”§ Soumettre des pull requests

---

## ğŸ“„ Licence

Ce projet est sous licence **MIT**. Voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

---

## ğŸ‘¤ Auteur

**Nedim Mejri**

- ğŸŒ GitHub : [@Nedim7050](https://github.com/Nedim7050)
- ğŸ“¦ Repository : [data-pipeline-streaming](https://github.com/Nedim7050/data-pipeline-streaming)

---

## â­ Support

Si ce projet vous a Ã©tÃ© utile, n'hÃ©sitez pas Ã  lui donner une â­ sur GitHub !

---

<div align="center">

**Fait avec â¤ï¸ par Nedim Mejri**

[â¬† Retour en haut](#-data-pipeline-streaming)

</div>
