# Data Pipeline Streaming

Pipeline de donnÃ©es de bout-en-bout illustrant la gÃ©nÃ©ration, l'ingestion, la transformation et la visualisation de transactions financiÃ¨res synthÃ©tiques.

## ğŸš€ DÃ©marrage Rapide

### Option 1: Version SimplifiÃ©e (Sans Docker) - **RECOMMANDÃ‰**

```powershell
# 1. Installer les dÃ©pendances
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -r requirements-simple.txt

# 2. CrÃ©er la base de donnÃ©es et gÃ©nÃ©rer des donnÃ©es
python create_database.py

# 3. Lancer le dashboard
streamlit run dashboard_working.py
```

### Option 2: Version ComplÃ¨te (Avec Docker)

```bash
# 1. DÃ©marrer l'Ã©cosystÃ¨me
docker compose up -d

# 2. Activer le DAG dans Airflow UI (http://localhost:8080)

# 3. Produire des Ã©vÃ©nements
python producer/producer.py --rows 10000 --rate 100

# 4. Visualiser
streamlit run analytics/streamlit_dashboard.py
```

## ğŸ“Š Architecture

- **Producteur** (`producer/producer.py`) gÃ©nÃ¨re des Ã©vÃ©nements JSON et les publie sur Kafka
- **Kafka & Zookeeper** gÃ¨rent le transport des Ã©vÃ©nements
- **Airflow** orchestre l'ETL micro-batch (Kafka â†’ Postgres) via le DAG `transactions_etl`
- **Postgres** stocke les tables raw/curated et les vues matÃ©rialisÃ©es
- **Streamlit** visualise en quasi temps rÃ©el les transactions agrÃ©gÃ©es

## ğŸŒ DÃ©ploiement

### DÃ©ployer sur Streamlit Cloud

1. **Pousser le projet sur GitHub**
2. **Aller sur [Streamlit Cloud](https://streamlit.io/cloud)**
3. **Connecter votre repository GitHub**
4. **Configurer le dÃ©ploiement:**
   - **Main file:** `streamlit_app.py` (recommandÃ©) ou `dashboard_working.py`
   - **Python version:** `3.11`
5. **DÃ©ployer!**

Voir [DEPLOY.md](DEPLOY.md) pour plus de dÃ©tails.

### DÃ©ployer sur GitHub

```bash
# 1. Initialiser Git
git init

# 2. Ajouter les fichiers
git add .

# 3. Commit
git commit -m "Initial commit"

# 4. CrÃ©er un repository sur GitHub et pousser
git remote add origin https://github.com/votre-username/data-pipeline-streaming.git
git branch -M main
git push -u origin main
```

## ğŸ“ Structure du Projet

```
data-pipeline-streaming/
â”œâ”€â”€ producer/          # GÃ©nÃ©rateur de transactions
â”œâ”€â”€ consumers/         # Consumer Kafka â†’ Postgres/SQLite
â”œâ”€â”€ airflow_dags/      # DAGs Airflow
â”œâ”€â”€ analytics/         # Dashboard Streamlit
â”œâ”€â”€ scripts/           # Scripts utilitaires
â”œâ”€â”€ sql/               # SchÃ©mas SQL
â”œâ”€â”€ docker/            # Configuration Docker
â”œâ”€â”€ requirements.txt   # DÃ©pendances (Docker)
â”œâ”€â”€ requirements-simple.txt  # DÃ©pendances (Local)
â””â”€â”€ README.md
```

## ğŸ”§ Configuration

### Variables d'environnement

```bash
# Kafka
KAFKA_BOOTSTRAP_SERVER=localhost:29092
KAFKA_TOPIC=transactions

# Postgres
POSTGRES_CONN_URI=postgresql+psycopg2://airflow:airflow@localhost:5432/transactions

# SQLite (Version simplifiÃ©e)
SQLITE_DB_PATH=data/transactions.db
```

## ğŸ“Š FonctionnalitÃ©s

- âœ… GÃ©nÃ©ration de transactions synthÃ©tiques
- âœ… Ingestion Kafka ou fichiers JSONL
- âœ… Transformation ETL (Airflow ou script Python)
- âœ… Stockage Postgres ou SQLite
- âœ… Visualisation Streamlit avec graphiques et mÃ©triques
- âœ… Export CSV pour PowerBI/Tableau

## ğŸ“š Documentation

- [QUICKSTART.md](QUICKSTART.md) - DÃ©marrage rapide
- [DEPLOY.md](DEPLOY.md) - Guide de dÃ©ploiement
- [GITHUB-DEPLOY.md](GITHUB-DEPLOY.md) - Guide complet GitHub et Streamlit Cloud
- [README-CLOUD.md](README-CLOUD.md) - DÃ©ploiement cloud
- [START-HERE.md](START-HERE.md) - Guide de dÃ©marrage dÃ©taillÃ©

## ğŸ“„ Licence

Projet sous licence MIT (voir `LICENSE`).
