# data-pipeline-streaming - Version SimplifiÃ©e (Sans Docker)

Cette version simplifiÃ©e fonctionne **sans Docker** en utilisant :
- **SQLite** au lieu de Postgres
- **Fichiers JSONL** au lieu de Kafka
- **Script Python simple** au lieu d'Airflow

## ğŸš€ DÃ©marrage Rapide

### 1. Installation des dÃ©pendances

```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -r requirements-simple.txt
```

### 2. GÃ©nÃ©rer des donnÃ©es de test

```powershell
python producer/producer_to_file.py --rows 10000 --rate 100 --output data/queue/transactions.jsonl
```

### 3. Traiter les donnÃ©es (ETL)

**Option A : Traitement manuel (une fois)**
```powershell
python consumers/file_queue_to_sqlite.py --input data/queue/transactions.jsonl --db data/transactions.db --batch-size 500
```

**Option B : Traitement automatique (scheduler simple)**
```powershell
python scripts/run_simple_etl.py data/queue/transactions.jsonl data/transactions.db 500 30
```
Ce script traite les donnÃ©es toutes les 30 secondes.

### 4. Visualiser avec Streamlit

```powershell
streamlit run analytics/streamlit_dashboard_sqlite.py
```

Ouvrez http://localhost:8501 dans votre navigateur.

## ğŸ“ Structure des Fichiers

```
data/
â”œâ”€â”€ queue/
â”‚   â””â”€â”€ transactions.jsonl    # Fichier d'entrÃ©e (simule Kafka)
â””â”€â”€ transactions.db            # Base SQLite (simule Postgres)
```

## ğŸ”„ Workflow Complet

1. **GÃ©nÃ©rer des transactions** â†’ `producer/producer_to_file.py`
2. **Traiter les transactions** â†’ `consumers/file_queue_to_sqlite.py` ou `scripts/run_simple_etl.py`
3. **Visualiser** â†’ `analytics/streamlit_dashboard_sqlite.py`

## ğŸ“Š Exporter les DonnÃ©es

Le dashboard Streamlit permet d'exporter un CSV pour PowerBI/Tableau.

## ğŸ”§ Configuration

Vous pouvez modifier les chemins par dÃ©faut via des variables d'environnement :

```powershell
$env:SQLITE_DB_PATH = "data/transactions.db"
$env:INPUT_FILE = "data/queue/transactions.jsonl"
```

## ğŸ†š Comparaison avec la Version ComplÃ¨te

| Composant | Version ComplÃ¨te | Version SimplifiÃ©e |
|-----------|------------------|-------------------|
| Message Queue | Kafka | Fichier JSONL |
| Base de donnÃ©es | Postgres | SQLite |
| Orchestration | Airflow | Script Python |
| DÃ©ploiement | Docker Compose | Local |

## ğŸ“ Notes

- La version simplifiÃ©e est idÃ©ale pour le dÃ©veloppement et les tests locaux
- Pour la production, utilisez la version complÃ¨te avec Docker ou des services cloud
- SQLite peut gÃ©rer des millions de lignes, mais Postgres est recommandÃ© pour de gros volumes

## ğŸš€ DÃ©ploiement sur Streamlit Cloud

Pour dÃ©ployer le dashboard sur Streamlit Cloud :

1. **Pousser votre code sur GitHub**
2. **CrÃ©er une app sur https://share.streamlit.io**
3. **Configurer les secrets** (si vous utilisez Postgres cloud) :
   ```
   POSTGRES_CONN_URI = postgresql://user:password@host:5432/db
   ```
4. **Point d'entrÃ©e** : `analytics/streamlit_dashboard_sqlite.py`

**Note** : Streamlit Cloud ne peut pas accÃ©der Ã  des fichiers locaux. Pour dÃ©ployer cette version, vous devrez :
- Soit utiliser une base Postgres cloud (voir section "Services Cloud" ci-dessous)
- Soit hÃ©berger SQLite sur un service de stockage accessible (S3, etc.)


