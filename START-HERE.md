# üöÄ Guide de D√©marrage Rapide - Windows (Sans Docker)

Bienvenue ! Ce guide vous permet de d√©marrer votre pipeline de donn√©es **sans installer Docker**.

## ‚ö° D√©marrage en 5 Minutes

### √âtape 1 : Installer Python (si pas d√©j√† fait)

1. T√©l√©charger Python 3.11+ depuis https://www.python.org/downloads/
2. **Important** : Cocher "Add Python to PATH" lors de l'installation
3. V√©rifier l'installation :
   ```powershell
   python --version
   ```

### √âtape 2 : Installer les D√©pendances

#### Option A : Installation Automatique (Recommand√©)

```powershell
# Utiliser le script d'installation automatique
.\scripts\install_windows.ps1
```

#### Option B : Installation Manuelle

**Si vous utilisez Python 3.11 ou 3.12** (recommand√©) :
```powershell
# Cr√©er un environnement virtuel
python -m venv .venv

# Activer l'environnement virtuel
.\.venv\Scripts\Activate.ps1

# Installer les d√©pendances
pip install -r requirements-simple.txt
```

**Si vous utilisez Python 3.14 alpha** :
```powershell
# Cr√©er un environnement virtuel
python -m venv .venv

# Activer l'environnement virtuel
.\.venv\Scripts\Activate.ps1

# Installer Pillow et PyArrow d'abord (binaires pour Python 3.14)
pip install --upgrade pip
pip install pillow --upgrade --only-binary :all:
pip install pyarrow --only-binary :all: --no-deps

# Installer les autres d√©pendances
pip install sqlalchemy pandas streamlit python-dotenv click tqdm altair greenlet --no-deps
pip install altair blinker cachetools packaging protobuf requests tenacity toml typing-extensions watchdog gitpython pydeck tornado numpy python-dateutil pytz tzdata colorama jinja2 jsonschema narwhals gitdb six charset-normalizer idna urllib3 certifi smmap markupsafe attrs jsonschema-specifications referencing rpds-py mdurl pygments
pip install greenlet "altair<6,>=4.0"
```

üí° **Note** : Si vous utilisez Python 3.14 alpha, vous devrez peut-√™tre installer certaines d√©pendances manuellement. Consultez la section "Probl√®mes Courants" ci-dessous.

### √âtape 3 : Tester l'Installation

```powershell
# Ex√©cuter les tests
python scripts/test_simple.py
```

Ou utiliser le script PowerShell :
```powershell
.\scripts\test_simple.ps1
```

### √âtape 4 : G√©n√©rer des Donn√©es

```powershell
python producer/producer_to_file.py --rows 10000 --rate 100
```

Cela g√©n√®re 10 000 transactions dans `data/queue/transactions.jsonl`.

### √âtape 5 : Traiter les Donn√©es

**‚úÖ Solution Simple (Recommand√©)**

```powershell
python create_database.py
```

Ce script va cr√©er la base de donn√©es et ins√©rer les donn√©es automatiquement.

**Option B: Utiliser le Consumer Directement**

```powershell
python consumers/file_queue_to_sqlite.py --input data/queue/transactions.jsonl --db data/transactions.db --batch-size 500
```

### √âtape 6 : Visualiser avec Streamlit

```powershell
streamlit run analytics/streamlit_dashboard_sqlite.py
```

Ouvrez http://localhost:8501 dans votre navigateur.

## üéØ Workflow Complet

### Option A : Traitement Manuel

1. **G√©n√©rer des donn√©es** :
   ```powershell
   python producer/producer_to_file.py --rows 10000 --rate 100
   ```

2. **Traiter les donn√©es** :
   ```powershell
   python consumers/file_queue_to_sqlite.py --input data/queue/transactions.jsonl --db data/transactions.db
   ```

3. **Visualiser** :
   ```powershell
   streamlit run analytics/streamlit_dashboard_sqlite.py
   ```

### Option B : Traitement Automatique (Scheduler)

Pour traiter automatiquement les donn√©es toutes les 30 secondes :

```powershell
python scripts/run_simple_etl.py data/queue/transactions.jsonl data/transactions.db 500 30
```

Dans un autre terminal, g√©n√©rez des donn√©es en continu :

```powershell
python producer/producer_to_file.py --rows 0 --rate 50
```

## üìä Fonctionnalit√©s du Dashboard

Le dashboard Streamlit affiche :
- ‚úÖ Derni√®res transactions
- ‚úÖ Volume horaire (graphiques)
- ‚úÖ Top marchands
- ‚úÖ Heatmap Ville vs Cat√©gorie
- ‚úÖ Export CSV pour PowerBI/Tableau

## üîß Configuration

Vous pouvez modifier les chemins par d√©faut via des variables d'environnement :

```powershell
$env:SQLITE_DB_PATH = "data/transactions.db"
$env:INPUT_FILE = "data/queue/transactions.jsonl"
```

## üìÅ Structure des Fichiers

```
data/
‚îú‚îÄ‚îÄ queue/
‚îÇ   ‚îî‚îÄ‚îÄ transactions.jsonl    # Fichier d'entr√©e (simule Kafka)
‚îî‚îÄ‚îÄ transactions.db            # Base SQLite (simule Postgres)
```

## üÜò R√©solution de Probl√®mes

### Erreur : "Python n'est pas reconnu"
- V√©rifier que Python est install√©
- V√©rifier que Python est dans le PATH
- Red√©marrer PowerShell

### Erreur : "Module not found"
- Activer l'environnement virtuel : `.\.venv\Scripts\Activate.ps1`
- Installer les d√©pendances : `pip install -r requirements-simple.txt`

### Erreur : "Permission denied"
- Ex√©cuter PowerShell en tant qu'administrateur
- V√©rifier les permissions du dossier `data/`

### Dashboard ne charge pas
- V√©rifier que la base SQLite existe : `data/transactions.db`
- V√©rifier que des donn√©es ont √©t√© trait√©es
- V√©rifier les logs dans la console

## üìö Ressources

- [QUICKSTART.md](QUICKSTART.md) - D√©marrage rapide
- [DEPLOY.md](DEPLOY.md) - Guide de d√©ploiement
- [README.md](README.md) - Documentation compl√®te du projet

## üéâ Prochaines √âtapes

1. ‚úÖ Tester la version simplifi√©e localement
2. üîÑ Explorer le dashboard Streamlit
3. ‚òÅÔ∏è D√©ployer sur Streamlit Cloud (voir [DEPLOY.md](DEPLOY.md))
4. üöÄ Passer √† la version compl√®te avec Docker (voir [README.md](README.md))

## üí° Conseils

- **G√©n√©rer plus de donn√©es** : Augmentez `--rows` pour plus de transactions
- **Traiter en continu** : Utilisez `scripts/run_simple_etl.py` avec un intervalle court
- **Exporter les donn√©es** : Utilisez le bouton "Export CSV" dans le dashboard
- **Explorer les donn√©es** : Utilisez le notebook `notebooks/exploration.ipynb`

---

**Besoin d'aide ?** Consultez les autres fichiers README ou ouvrez une issue sur GitHub.


